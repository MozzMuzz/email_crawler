---
title: "01_CollectHTML"
author: "Moritz Muller"
date: "February 14, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This project collects all html from websites and searches the files for email addresses and appropriate tags. In principle, all websites can be used. In our case, we only select websites of interest groups to use the email addresses to send out a large-n survey

# Load requires packages
```{r}
library(Rcrawler)
library(stringr)
```

# First trial run with single website: download all html of website, clean the HTML and look for regex matches for email
```{r}
Rcrawler(Website = "http://www.edf-feph.org/", no_cores = 4, no_conn = 4, MaxDepth = 1, DIR = "./data")

rawHTML <- paste(readLines("./data/edf-feph.org-141609/6 .html "), collapse="\n")
#write html cleaner function
cleanHTML <- function(x){
  x <- gsub("<.*?>", "", x)
  x <- gsub("\n", "", x)
  x <- gsub("\t", "", x)
  x <- gsub("@import", "", x)
}
rawHTML <- cleanHTML(x = rawHTML)
str_extract_all(rawHTML, "[[:alnum:].-_]+@[[:alnum:].-]+.[a-z]{2,4}")


```

