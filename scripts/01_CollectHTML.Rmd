---
title: "01_CollectHTML"
author: "Moritz Muller"
date: "February 14, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This project collects all html from websites and searches the files for email addresses and appropriate tags. In principle, all websites can be used. In our case, we only select websites of interest groups to use the email addresses to send out a large-n survey

# Load requires packages
```{r}
library(Rcrawler)
library(stringr)
library(dplyr)
```

# First trial run with single website: download all html of website, clean the HTML and look for regex matches for email
```{r}
Rcrawler(Website = "https://tporganics.eu/", no_cores = 4, no_conn = 4, MaxDepth = 1, DIR = "./data/scraped")


folders.website <- data.frame(list.files("./data/scraped"))


results <- list(list())
recommended.emails <- list()

for(i in 1:nrow(folders.website)){ 
  files.html <- data.frame(list.files(paste0("./data/scraped/",folders.website[i,])))
  URL <- as.character(folders.website[i,])
  for (l in 1:nrow(files.html)){
    rawHTML <- paste(readLines(paste0("./data/scraped/", folders.website[i,],"/", files.html[l,])), collapse="\n")
    results[[URL]][l] <- as.data.frame(str_extract_all(rawHTML, "[a-zA-Z][a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+.[a-zA-Z0-9-.]+"))
  }
  temporary.stash <- as.data.frame(unlist(results[[URL]]))
  names(temporary.stash)[1] <- "emails"
  most_probable <- temporary.stash%>%
    count(emails)%>%
    top_n(3,n)
  most_probable <- most_probable[1:5,]
  recommended.emails[[i]] <- most_probable[order(most_probable$n, decreasing = T),]
  write.csv(temporary.stash, file = paste0("./data/scraped/", folders.website[i,],"_emails.csv"))
}






#alternative: [[:alnum:].-_]+@[[:alnum:].-]+.[a-z]{2,4}
```
Now do this on a big scale: First loop through all files and get email addresses. Then create smart database that returns most probable email address

```{r}
# Get all the data!
websites <- read.csv("./data/websites.csv", stringsAsFactors = F)
websites <- websites[1:100,]

for (i in websites$Website){ 
  tryCatch({
    Rcrawler(Website =i, no_cores = 4, no_conn = 4, MaxDepth = 1, DIR = "./data/scraped")
}, error = function(e){cat("ERROR: ", conditionMessage(e), "\n")})
}


 


#now extract addresses
folders.website <- data.frame(list.files("./data/test"))
results <- list(list())
recommended.emails <- list()

for(i in 1:nrow(folders.website)){ 
  files.html <- data.frame(list.files(paste0("./data/test/",folders.website[i,])))
  URL <- as.character(folders.website[i,])
  for (l in 1:nrow(files.html)){
    rawHTML <- paste(readLines(paste0("./data/test/", folders.website[i,],"/", files.html[l,])), collapse="\n")
    results[[URL]][l] <- as.data.frame(str_extract_all(rawHTML, "[a-zA-Z][a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+.[a-zA-Z0-9-.]+"))
  }
  temporary.stash <- as.data.frame(unlist(results[[URL]]))
  names(temporary.stash)[1] <- "emails"
  most_probable <- temporary.stash%>%
    count(emails)%>%
    top_n(3,n)
  most_probable <- most_probable[1:5,]
  recommended.emails[[i]] <- most_probable[order(most_probable$n, decreasing = T),]
  write.csv(temporary.stash, file = paste0("./data/test/", folders.website[i,],"_emails.csv"))
}


#parallized version of the loop, because it takes a while
library(foreach)
library(doParallel)

registerDoParallel(cores=4)
stopCluster()

```
Deprecated things
```{r}
#Html cleaner (messed with white spaces, because it deleted them)
#write html cleaner function
cleanHTML <- function(x){
  x <- paste(x, collapse="\n")
  x <- gsub("<.*?>", " ", x)
  x <- gsub("\n", " ", x)
  x <- gsub("\t", " ", x)
  x <- gsub("@import", " ", x)
}
rawHTML <- cleanHTML(x = rawHTML)
view <- as.data.frame(rawHTML)
```

